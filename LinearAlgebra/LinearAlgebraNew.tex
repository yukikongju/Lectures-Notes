\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{parskip}
\usepackage{textgreek}
\begin{document}
\title{Notes de Cours - Algèbre Linéaire}
\author{Emulie Chhor}
\maketitle


\section*{Introduction}

Le cours d'algèbre linéaire comporte plusieurs chapitres:

    \begin{enumerate}
	\item Systèmes d'Équations Linéaires
	\item Matrices
	\item Déterminants
	\item Espaces et Sous-Espaces Vectoriels
	\item Orthogonalité et Projections
	\item Diagonalisation
    \end{enumerate}

\newtheorem{definition}{Definition}[subsection]
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{corollary}{Corollary}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{axiom}{Axiome}
\newtheorem{property}{Propriété}[subsection]
\newtheorem*{remark}{Remarque}
\newtheorem*{problem}{Problème}
\newtheorem*{intuition}{Intuition}

\subsection{Pourquoi étudier l'algèbre linéaire?}

\pagebreak

\section{Systèmes d'Équations Linéaires}

\subsection{Overview}

Le but de cette section est de pouvoir résoudre des systèmes d'équations linéaires
avec l'algorithme de Gauss et de Gauss-Jordan

\subsection{Définition d'un système d'équations linéaires}
\subsection{Opérations élémentaires sur les lignes}

Il existe 3 opérations élémentaires sur les matrices:
\begin{enumerate}
    \item Multiplier la ligne i par une constante $k \neq 0$ : $L_i \rightarrow
	k L_j , k \neq 0$
    \item Permuter les lignes i et j : $ L_i \leftrightarrow L_j$
    \item Ajouter à la linge i un multiple d de la ligne j : $ L_i \rightarrow
	L_i + d L_j$
\end{enumerate}

\subsection{Forme des matrices}

Lorsqu'on essait de résoudre un SEL, on veut transformer notre matrice en l'une
des deux formes:
\begin{enumerate}
    \item Forme échelonée : Gauss
    \item Forme échelonée réduite : Gauss-Jordan
\end{enumerate}

\subsection{Types de Solutions}

Il existe 3 types de solutions:
\begin{enumerate}
    \item Solution Unique: chaque variable est associée à un pivot
    \item Infinité de Solutions: il existe une variable libre ou plus
    \item Aucune Solutions: le système est incompatible et on a
	$ [0 \, 0 \, ... \, 0 | \, k], \, k \neq 0$
\end{enumerate}

\subsection{Méthode de Gauss}
\subsection{Méthode de Gauss-Jordan}

\begin{remark}[Méthode de Gauss vs Gauss-Jordan]
\end{remark}

\pagebreak

\section{Matrices}
\subsection{Overview}

Le but de cette section est de se familiariser algébriquement avec la notion de
matrice. Cependant, il faut toujours garder en tête qu'une matrice représente
une transformation linéaire. Essentiellement, une transformation linéaire est
une fonction qui prend un vecteur et en recrache un autre. On verra plus tard
les détails.

\subsection{Opérations Matricielles}

\subsubsection{Addition et Multiplication par un scalaire}

\begin{definition}[Égalité Matricielle]
    On dit que 2 matrices $A_{mxn}$ et $B_{pxq}$ sont égales si:
    \begin{enumerate}
	\item meme dimension: m=p et n=q
	\item composantes identiques: $(A)_ij = (B)ij, \forall i,j$
    \end{enumerate}
\end{definition}

\begin{definition}[Addition Matricielle]
    Soit A et B, deux matrices de même dimension. La Matrice A+B est
    de même dimension et est caractérisée par $$(A+B)_ij = (A)_ij + (B)_ij$$
\end{definition}

\begin{remark}[Interprétation Géométrique de l'addition Matricielle]

\end{remark}

\begin{defintion}[Multiplication par un scalaire]
    Soit A, une matrice, et r un scalaire. La matrice rA, de meme dimension,
    est donnée par $$ (rA)_ij = r(A)_ij$$
\end{defintion}

\begin{remark}[Interprétation Géométrique de la multiplication Matricielle]

\end{remark}

\begin{theorem}[Propriétés de la somme et du PPS matriciel]
    Soit A,B,C des matrices et r,s des scalaires.
    \begin{enumerate}
	\item (Commutativité de l'addition) : A + B = B + A
	\item (Associativité de l'addition) : (A+B)+C=A+(B+C)
	\item (Identité de la somme) : A + 0 = A
	\item r(A+B) = rA + rB
	\item (r+s)A = rA + sA
	\item r(sA) = (rs)A
    \end{enumerate}
\end{theorem}

\begin{remark}
    Algébriquement, on sait que les matrices possèdent ces propriétés,
    car ses composantes sont des réels ou des complexes, qui sont
    des fields. Géométriquement, on sait que les matrices sont construites
    à partir de vecteurs, alors il s'agit encore de d'addition et de
    multiplication, mais sur des vecteurs au lieu de composantes.
\end{remark}

\subsubsection{Multiplication Matricielle}

\begin{definition}[Multiplication Matricielle]
    Lignes x Colonnes
\end{definition}

\begin{theorem}[Propriétés de la Multiplication Matricielle]
    Soit A,B,C, des matrices carrées et r un scalaire. Alors,
    \begin{enumerate}
	\item $ AB \neq BA$
	\item $ r(AB) = (rA)B = A(rB), r \in \mathbb{R}$
	\item $ IA = AI = A$
	\item $A(B+C) = AB + AC$
	\item $(B+C)A=BA + CA$
	\item $A(BC) = (AB)C$
    \end{enumerate}
\end{theorem}

\begin{remark}[Interprétation géométrique]
    Géométriquement, on peut interpréter la multiplication matricielle
    comme la composition de transformations linéaires, ce qui fait en
    sorte que l'ordre est important.\\
    Que peut-on dire à propos de A et B si AB=BA?
\end{remark}

\subsubsection{Transposition de Matrice}

\begin{definition}[Transposition]
    La transposition de la matrice A, notée $A^T$, est la matrice dont on
    a interchangé ses lignes et ses colonnes. $$ (A^T)_{ij} = (A)_{ji}$$
\end{definition}

\begin{theorem}[Propriétés de la Transposée matricielle]
    Soit A,B,C, des matrices carrées et r un scalaire. Alors,
    \begin{enumerate}
	\item $(A^T)^T = A$
	\item $ (A+B)^T = A^T + B^T$
	\item $ (rA)^T = r(A^T), r \in \mathbb{R}$
	\item $ (AB)^T = B^T A^T$
    \end{enumerate}
\end{theorem}

\subsection{Équation Ax=b}

\subsubsection{Combinaison Linéaire}

\begin{definition}[Combinaison Linéaire]
    On dit que le vecteur u est une combinaison linéaire des vecteurs
    $v_1, v_2, v_3$ dans l'espace vectoriel V s'il existe des scalaires
    $c_2, c_2, c_3$ tels que: $$ u = c_1 v_1 + c_2 v_2 + ... + c_n v_n$$
\end{definition}

\begin{intuition}[Interprétation Géométrique de la Combinaison Linéaire]
    On veut exprimer un vecteur quelconque comme une somme de plusieurs
    autres. C'est comme si on voulait se rendre à la destination X, mais
    qu'on s'arrêtait à pleins d'endroits.
\end{intuition}

\subsubsection{Indépendance Linéaire}

\begin{definition}[Indépendance Linéaire]
    On dit que la famille des vecteurs $(v_1, v_2, ..., v_n)$ de l'espace
    vectoriel sont linéairement indépendants $\Longleftrightarrow
    c_1 = c_2 = ... = c_n = 0$ est la seule solution pour la combinaison
    linéaire $ c_1 v_1 + c_2 v_2 + ... + c_n v_n = 0$
\end{definition}

\begin{intuition}
    On ne peut pas écrire les vecteurs de l'ensemble comme une combinaison
    linéaire des autres. Plus tard, on verra que l'indépendance linéaire
    fait en sorte qu'on a un minimum de vecteurs nécessaires pour générer
    un ensemble de vecteurs.
\end{intuition}

\subsubsection{Équation Ax=b}

\begin{theorem}[Compatibilité de Ax=b]
    Le système Ax=b est compatible si et seulement si b est une combinaison
    linéaire des colonnes de A
\end{theorem}

\begin{intuition}
    On sait que A est une matrice de transformation, ce qui veut dire que b
    est le vecteur x après avoir appliqué cette transformation. Ainsi,
    on peut s'imaginer que les colonnes de A ont été utilisées pour faire
    le doit product, qui nous donne l'étirement de chaque composantes de x.
    Cet étirement peut donc être vu comme un scalaire par lequel on
    multiplie x pour obtenir le vecteur b.
\end{intuition}

\begin{theorem}
    Le système Ax=0 possède une unique solution si et seulement si les
    colonnes de A sont linéairement indépendantes.
\end{theorem}

\begin{intuition}
    On veut multiplier les vecteurs colonnes de A par le vecteur nul pour
    obtenir zéro, ce qui est équivalent à l'indépendance linéaire.
\end{intuition}

\subsection{Inversion de Matrices}

\subsubsection{Définition et Propriétés de l'inverse}

\begin{definition}[Matrice Inversible]
    On dit qu'une matrice carrée est inversible s'il existe une matrice B
    telle que $AB=BA=I$
\end{definition}

\begin{theorem}[Unicité de l'inverse]
    Si A est une matrice inversible, alors elle est l'inverse de A, notée
    $A^(-1)$, est unique
\end{theorem}

\begin{remark}[Interprétation de l'inverse matricielle]
    L'inverse d'une matrice est l'opération qui annule la transformation
    faite par la matrice A. On sait que l'inverse existe si la
    transformation ne change pas la dimension du vecteur/matrice. On
    verra plus tard pourquoi les matrices ayant un détarminant nul ne
    sont pas inversibles.
\end{remark}

\subsubsection{Calcul de l'inverse avec la méthode de Gauss-Jordan}

\begin{definition}[Algorithme de Gauss-Jordan]
    On peut calculer l'inverse d'une matrice si $$ (A \, | \, I) ~ ...
    ~ (\, I \, | \, A^(-1))$$
\end{definition}

\begin{intuition}
    On est capable de trouver l'inverse avec Gauss-Jordan puisque chaque
    opération matricielle peut être associée à une matrice élémentaire.
    C'est comme si on faisait une composition de matrices élémentaires.
    On verra plus tard pourquoi les matrices élémentaires sont inversibles.
\end{intuition}

\subsubsection{Résolution du système Ax=b avec l'inverse}

On peut trouver le vecteur x à l'aide de l'inverse de la façon suivante:
\begin{equation*}
    $ Ax = B$ \\ $ A^(-1) (Ax) = A^(-1)b$ \\ $ (A^(-1) A)x = A^(-1)b $ \\
    $ Ix = A^(-1)b $\\ $ x = A^(-1)b$
\end{equation*}


\subsubsection{Matrices élémentaires}






\pagebreak
\section{Déterminants}
\subsection{Overview}

Géométriquement, le déterminant d'une matrice représente le ratio entre l'aire
formée par les vecteurs avant et après transormations linéaires. Intuitivement,
le déterminant nous dit si une matrice est diagonalisable ou non puisque si
$det(A)=0$, alors la transformation à changer de dimension et n'est pas inversible.

On peut aussi utiliser les déterminants pour évaluer un SEL. On compare le déterminant
de chaque vecteur avant et après transformation.

\subsection{Définition du Déterminant}
\subsection{Propriétés du Déterminant}
\subsection{Règle de Cramer}

\pagebreak

\section{Espaces et Sous-Espaces Vectoriels}
\subsection{Overview}

Un aspect important à comprendre dans l'étude de l'algèbre linéaire est la notion
d'espace. Malgré qu'on s'attarde peu à la notion d'anneau, il faut se souvenir
qu'on peut manipuler les vecteurs algébriquement, car l'espace dans lequel il se
trouve est un field, un ensemble qui comporte certaines propriétés qu'on nomme
axiomes.\\

Un deuxième aspect important est la notion de transformation linéaire. Comme mentionné
plus tôt, une matrice peut être interprétée comme une transformation, c-à-d qu'elle
est une fonction qui transforme un vecteur en un autre vecteur. Cependant, cette
transformation peut changer les propriétés de l'espace. On s'attardera donc aux
transformations qui gardent ces propriétés, transformations qu'on nomme linéaire.

Finalement, on veut être capable de changer de base. Décrire un vecteur dans
l'espace est assez arbitraire, car chaque personne peut tracer son propre quadrillage
et obtenir un vecteur différent. Ainsi, pour s'assurer de parler le même language,
on veut être capable de traduire un vecteur/transformation d'un espace à un autre.

\subsection{Espaces vectoriels sur $\mathbb{R}$}
\subsection{Transformations Linéaires}
\subsection{Base d'un Espace Vectoriel}
\subsection{Système de Coordonnées dans $\mathbb{R}^n$}
\subsection{Changement de Base}

\pagebreak
\section{Orthogonalité et Projections}
\subsection{Overview}

\subsection{Orthogonalité et Projection}
\subsection{Sous-Espace Orthogonal}

\pagebreak

\section{Diagonalisation}
\subsection{Overview}

La diagonalisation est un processus utilisé dans le domaine du Machine Learning,
car elle permet de "compresser" les données dans une plus petite dimension. La
diagonalisation est notamment utilisée pour le PCA et le SVD.\\

Géométriquement, lorsqu'on parle de valeurs propres et de vecteurs propres, c'est
qu'il existe un vecteur qui reste sur le même axe après transformation linéaire.
Ainsi, appliquer la transformation linéaire sur ce vecteur peut être considérer
comme une multiplication par un scalaire $\lambda$, qui étire la longueur du vecteur.
Notre but est donc de trouver le vecteur qui reste dans le même axe, qu'on appelle
vecteur propre, et de trouver le facteur de multiplication, qu'on appele valeur
propre.

\subsection{Valeurs et Vecteurs Propres}
\subsection{Espace Propre et Multiplicité Géométrique}
\subsection{Diagonalisation d'une matrice quelconque}
\subsection{Diagonalisation d'une matrice symétrique}

\pagebreak

\end{document}
\end{article}
