\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{parskip}
\usepackage{textgreek}
\begin{document}
\title{Résumé des Cours - Processus Stochastiques}
\author{Emulie Chhor}
\maketitle

\section*{Introduction}

Ce document est un résumé des idées importantes présentées lors de chaque cours
de MAT2717

\newtheorem{definition}{Definition}[subsection]
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{corollary}{Corollary}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{axiom}{Axiome}
\newtheorem{property}{Propriété}[subsection]
\newtheorem*{remark}{Remarque}
\newtheorem*{problem}{Problème}
\newtheorem*{intuition}{Intuition}

% \renewcommand*{idea}{Big Big Idea}
% \renewcommand*{outline}{Outline}

\pagebreak
\section{Semaine 1}
\subsection{Théorie 1}

\subsection*{Big Idea}

Dans le premier cours de probabilité, on s'est penché sur la notion de variable
aléatoire. Dans ce cours-ci, on voit qu'on doit ajouter un autre paramètre à
ces variables aléatoires: le temps. On ne parle donc plus de variables aléaoires,
mais de processus stochastiques, qui sont des variables aléatoires prenant en
considération le temps. Ainsi, on parle donc de processus stochastique à temps
discrete et à temps continu, dépendemment si on considère le temps discret ou
continu.

\subsection*{Outline}
\begin{enumerate}
    \item Processus Stochastiques
    \item Marche Aléatoire
\end{enumerate}

\subsection{Processus Stochastiques}

Un processus stochastique est une v.a aléatoire qui est dépendante du temps.
Elle peut être discrète ou continue dépendemment à quelle fréquence on
observe la valeur de la v.a)\\

On dit que le temps est déterministe. Why?\\

On peut écrire un p.s. $X( \omega, t )$, avec omega: les scenarios
possibles et t, le temps. Si on fixe le temps, on parle de variable
aléatoire. Si on fixe l'évènement et on laisse le temps libre, on
parle de trajectoire.

\subsection{Marche Aléatoire}

La marche aléatoire représente la valeur d'un p.s. discret à un temps donnée.
On peut la modéliser de deux façons:
\begin{enumerate}
    \item Conditionnement: valeur à la dernière position $\pm$ 1
    \item Somme de variables indicatrices: $X_n = Y_1 + ... +Y_n$
\end{enumerate}

Il est à noter qu'on préfère représenter une marche aléatoire par une
somme de variable indicatrice puisque ça rend le calcul plus facile.

Espérance: $\mathbb{E} (X_n) = n (2p-1)$
Variance: $ Var(X_n)= 4np(1-p)$

\subsection{D'ou vient la formule de l'esperance et de la variance d'une
marche aleatoire}

Il s'agit de l'esperance et de la variance pour une somme de variable
indicatrice.

$E(x) = n(1 * p + (-1) (1-p))$ : nombre de pas x esperance succes/echec au
i-ème pas
$V(X)$ = somme des variances = $ n [ E(Y^2) - E(Y)^2 ]$

\subsection{Théorie 2}
\subsection*{Big Idea}

Ce cours introduisait les chaines de Markov. Comme on l'a vu au dernier
cours, les processus stochastiques sont des v.a. qui sont en fonction du
temps. Certaine PS ont la propriété de Markov, c-à-d qu'on n'a que besoin
de la dernière valeur pour déterminer la prochaine position et non tout
l'historique. On introduit la notion de matrice de transition pour modéliser
la probabilité de transitionner d'un état à l'autre, et la notion d'état
absorbant, qui est un état duquel on ne peut se sortir une fois atteinte.

\subsection*{Outline}

\begin{enumerate}
    \item Rappel: Processus Stochastiques
    \item Marche Aléatoire
    \item Processus de Branchement
    \item Chaines de Markov: Proba de transition et État absorbant
\end{enumerate}

\subsubsection{Marche Aléatoire}%
\label{ssub:marche_aléatoire}

Il faut se rappeler que la marche aléatoire sert à modéliser un p.s discret.
On a vu qu'il y avait deux façons de l'interpréter:
\begin{enumerate}
    \item Conditionnement: previous +/- 1 dépendemment du succès/échec
    \item Somme de variables indicatrices (bernouilli)
\end{enumerate}

Essentiellement, la probabilité est déterminer par une Bernouilli, puisqu'on
considère que chaque coin flip est iid. On peut aussi visualiser la
marche aléatoire comme un arbre de décision.\\

Exemple: t=2, coin flip: +1 si Head, -1 si Tail
\begin{enumerate}
    \item $x_2 = 2$ avec proba $p^2$ : {HH}
    \item $x_2 = 0$ avec proba $ 2p (1-p)$ : {HT, TH}
    \item $x_2 = -2$ avec proba $ (1-p)^2$ : {TT}
\end{enumerate}

Notons qu'après 2 coins flip, on ne peut pas avoir 1 ou -1, car apres le
premier coin flip, on est à 1 ou -1, et on ne peut que se déplacer de 1
vers le haut ou vers le bas

On a aussi vu qu'on peut tracer la trajectoire de la marche aléatoire, qui
représente la valeur de $x_n$ selon le temps.

\subsubsection{Processus de Branchement}%
\label{ssub:processus_de_branchement}

Le processus de Banchament est une façon de modéliser la générations
d'évènement. Par exemple, en reinforcement learning, ça représente la
transition d'état. Visuellement, on a un arbre et chaque branche mène
à un autre état, associé à une certaine probabilité.

\subsubsection{Chaine de Markov}%
\label{ssub:chaine_de_markov}

Comme mentionné, une chaine de Markov est une p.s qui satisfait à la
propriété de Markov, c-à-d qu'on n'a que besoin de l'état précédant pour
déterminer le futur et non tout l'historique. Dans la vraie vie, une
chaine de markov ne représente pas nécessairement la situation, mais
on fait cette hypothèse afin de simplifier les calculs et éviter de
trainer trop d'information. On écrit:
$$ \mathbb{P} (X_{n+1} = j) = (X_{n+1} = j | X_n + ... + X_0) =
(X_{n+1} = j | X_n) $$

Pour représenter la probabilité de transition d'un état à un autre, on
utilise une matrice de transition.

Finalement, il est important de comprendre la notion d'état absorbant.
Une fois qu'on entre dans cet état, on ne peut plus en sortir. L'exemple
donné en classe concernait le casino: une fois qu'on a plus d'argent à
gamble, on ne peut plus gagner d'argent et on reste à zéro.

Notons que si une chaine de Markov à un état absorbant, il est fort
probable qu'il y ait plusieurs états absorbant et qu'on puisse observer
un pattern.

\subsection{TP 1 - Rappel Probabilités et Chaines de Markov en Temps Discret}

Cette semaine, le TP se penchait sur 2 sujets:
\begin{enumerate}
    \item Rappel sur les concepts en probabilité
    \item Intro aux Processus Stochastiques
\end{enumerate}

\subsubsection{{Rappel sur les concepts en probabilité}}

\begin{enumerate}
    \item Principe Inclusion-Exclusion
    \item Probilités Totales pour généraliser le nombre de tirage avec et sans
	remise: $ P(A_1|B) P(B) + P(A_2|B)P(B) $
    \item Trouver la densité marginale d'une v.a: si continues, on doit essayer
	de retrouver une fonction de densité pour éviter d'avoir à résoudre
	l'intégrale
\end{enumerate}

\subsubsection{Intro aux Processus Stochastiques}

\begin{enumerate}
    \item Déterminer si une p.s. est Markov ou non
    \item Trouver/Lire une Matrice de Transition
    \item Déterminer la probabilité de transition d'un état à un autre
    \item Tracer le Graphe
\end{enumerate}

\subsection{Intro aux Processus Stochastiques}

\pagebreak

\section{Semaine 2}
\subsection{Théorie 1: Probabilité de Transition à plusieurs étapes}
\subsection*{Big Idea}

Ce cours-ci nous présentait certaines méthodologies pour déterminer
la probabilité de transition à plusieurs étapes pour des P.S discrèts
à temps homogène satisfaisant la propriété de Markov

\subsection*{Outline}
\begin{enumerate}
    \item Modèles: Marche aléatoire, Processus de Branchement, Modèle de
	diffusion de Gaz
    \item Probabilité de transition avec probabilité conditionnelle
    \item Probabilité de transition avec Relation Chapman-Kolmogrov
\end{enumerate}

\subsubsection{Modèles: Marche aléatoire, Processus de Branchement, Modèle de    diffusion de Gaz}

Dans un premier temps, on a réviser les notions de marche aléatoire,
processus de branchement et  modèle de diffusion de gaz. Essentiellement,
on voulait s'apercevoir que
\begin{enumerate}
    \item La valeur de $X_n$ ne peut que se déplacer de 1 (dans la plupart
	du temps)
    \item Conditionnement: on peut trouver la probabilité de transition
	d'un état à l'autre par conditionnement p/r à l'étape précédante
\end{enumerate}

\subsubsection{Probabilité de transition avec probabilité conditionnelle}

Dans les cours précédants, on nous donnait la matrice de transition pour
déterminer la probabilité de se déplacer d'un état à l'autre. Ici, on
voit 2 résultats importants:
\begin{enumerate}
    \item si P.S Markov et homogène (même proba peu import le temps),
	alors $$ P[X_2 = 3 , X_1 = 2 | X_0 = 1] = P[X_{n+2} = 3, X_{n+1} = 2
	| X_n =1] $$, ce qui veut dire que la proba de transition d'état ne
	dépend pas d'un temps quelconque, mais que de la "distance" (nombres
	de pas) entre 2 états, si P.S Markov est homogène
    \item On peut utiliser la probabilité conditionnelle pour déterminer la
	proba de transition entre 2 états
\end{enumerate}

\textbf{Preuve pour 1}

Puisqu'on a une chaine de Markov, les les 2 états sont indépendants et on peut
utiliser la règle de multiplication:

$$ P[X_2 = 3 , X_1 = 2 | X_0 = 1] $$
$$ = \frac{P(X_2 = 3, X_1 =2, X_0 = 1)}{P(X_0 = 1)}  $$
$$ = (\frac{P(X_2 = 3, X_1 =2, X_0 = 1)}{P(X_0 = 1)})
(\frac{P(X_1 = 3, X_1 = 2)}{P(X_0=1)} ) $$
$$ ... $$

\textbf{Probabilité Conditionnelle pour trouver la probabilité de transition}

Plus généralement, pour trouver la probabilité de transition à plusieurs
étapes, on doit conditionner par les étapes intermédiaires:

\begin{enumerate}
    \item 2 étapes: $ P(X_2 = 1 | X_0 =1) = \sum^{n}_{i=1} P(X_2 = 2, X_1 = k | X_0=1) $
    \item 3 étapes: $ P(X_3 = 1, X_2 = 1 | X_0 =1) = P(X_3 = 1, X_2 = 1)
	P(X_2 = 1 | X_1 =1) P(X_1 = 1 | X_0 =2)$
    \item m étapes: $ P(X_{n+m} = j | X_n = i) = P(n=j |X_0 = i) $
\end{enumerate}

\subsubsection{Probabilité de transition avec Relation Chapman-Kolmogrov}

La relation de Chapman-Kolmogrov est pratique, car elle nous permet de généraliser
le calcul pour la probabilité de transition en m étapes. Au lieu de multiplier
individuellement les intermédiaires à chaque étapes, on peut utiliser la
multiplication matricielle pour obtenir la matrice de transition après m
étapes. On note $ P^(m) = P^m $, P: matrice de transition.\\

Par exemple, si on veut la proba de transition à 3 étapes, on n'a qu'à faire
$$ P \times P \times P $$

\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item Preuve de la Relation Chapman-Kolmogrov
    \item Classes: Communication entre états
\end{enumerate}

\subsubsection{Preuve de la Relation Chapman-Kolmogrov}

La relation de Chapman-Kolmogov nous dit que si on a une chaine de
Markov à temps homogène, on peut multiplier les matrices de transition
pour obtenir la probabilité de passer de l'état n à l'état n+k
\begin{enumerate}
    \item $ P^{m}=P^m$
    \item $ P^{m+n}=P^m P^n$
\end{enumerate}

La preuve de cette relation se fait par induction, et on montre
l'étape d'induction avec la propriété des chaines de Markov et la
probabilité conditionnelle. Essentiellement, on veut montrer qu'en
multipliant chaque entrées entre elles, on obtient la proba à l'étape
n x la proba à l'étape 1 et qu'en multipliant ces matrices, on obtient
$ P^{n+1}$

\textbf{Cas de base}

\begin{enumerate}
    \item t=0: matrice identité, car on ne change pas d'état
    \item t=1: $P^1 = P$
\end{enumerate}

\textbf{Étape d'induction}

\begin{enumerate}
    \item Puisque CM est homogène dans le temps, alors
	$ P^{n+1} \Longrightarrow P_{ij} ^{n+1} = P[X_{n+1} = j | X_n = i]
	= P[X_{n+1} =j | X_0 = i] $
    \item En considèrant les états intermédiaires, on obtient
	$ P[X_{n+1}=j | X_0 = i] = \sum^{n}_{i=1} P[X_{n+1} = j | X_n = k]$
    \item Puisque c'est une chaine de Markov, on a que
	$$ \sum^{n}_{i=1} P[X_{n+1} = j, X_n = k | X_0 = i]$$
    \item Par probabilité conditionnelle, on a que
	$$ \sum^{n}_{i=1} \frac{P[X_{n+1} = j, X_n = k, X_0 = i]}
	{P[X_0=i]} \frac{P[X_n = k, X_0=i]}{P[X_n = k, X_0=i]} $$
    \item En réarrangeant les termes, on obtient la proba de transition
	à l'étape n et en une étape
	$$ \sum^{n}_{i=1} \frac{P[X_{n+1} = j, X_n = k, X_0 = i]}
	{P[X_n = k, X_0=i]} \frac{P[X_n = k, X_0=i]}{P[X_0=i]} $$
    \item et on applique la définition de la multiplication matricielle
\end{enumerate}

\subsubsection{Classes: Communication entre états}

La deuxième partie du cours mettait l'emphase sur la séparation des
états par des classes. Essentiellement, on veut être capable de
différentier les différentes classes dans une matrice de transition
afin de déterminer si la matrice est irréductible ou non.

\textbf{Qu'est-ce qu'une classe}

Tous les états qui communiquent entre eux forment une classe. Pour que
2 états communiquent entre entre elles, il faut qu'elles satisfassent
les conditions suivantes:
\begin{enumerate}
    \item Réflexive: $ i \leftrightarrow i$
    \item Symétrique: $ i \leftrightarrow j \Longleftrightarrow j
	\leftrightarrow i $
    \item Transitive: $ i \to j, j \to k \Longrightarrow i \to k $
\end{enumerate}

\textbf{Quelles sont les types de classes}

\begin{enumerate}
    \item État absorbant: On ne peut jamais quitter cet état après y
	être entré
    \item État de non-retour: une fois qu'on n'a quitté cet état,
	on ne peut jamais revenir
\end{enumerate}

\textbf{Qu'est-ce qu'une matrice irréductible}

On dit qu'une matrice est irréductible si tous les états communiquent
entre elles. Autrement dit, tous les états forment une seule classe.

\subsection{TP 2 - Classification des états}

Le TP de cette semaine se penchait sur 2 buts:
\begin{enumerate}
    \item Déterminer la probabilité de passer d'un état à l'autre
    \item Classification des états
\end{enumerate}

\subsection{Déterminer la probabilité de passer d'un état à l'autre}

Pour déterminer la probabilité de passer d'un état à l'autre, on a deux
façons:
\begin{enumerate}
    \item si le nombre de steps est relativement petit: conditionnement
	(tracer l'arbre peut aider)
    \item si le nombre de steps est grand: utiliser la relation de
	Chapman-Kolmogrov pour calculer la matrice de transition en
	diagonalisant
\end{enumerate}

\subsection{Classification des états}

Dans une matrice de transition, on veut être capable de regrouper les
états similaires afin de créer de classes. Tous les états dans une
même classe devraient pouvoir être accessible, c-à-d qu'il existe un
chemin entre les 2. Les classes peuvent être:
\begin{enumerate}
    \item Transition vs Récurrent: on pourrait quitter la classe et ne
	jamais y revenir vs une fois qu'on est dans cette classe, on
	ne peut jamais la quitter (non-retour)
    \item Périodique vs Apériodique: après combien de pas en revient
	à l'état d'où on est partit (cycle) vs pgcd = 1
    \item Irréductibilité: le graphe forme une seule classe
\end{enumerate}

\begin{remark}
    Il est utile de tracer le graphe pour déterminer les classes
\end{remark}

\pagebreak
\section{Semaine 3}
\subsection{Théorie 1}
\subsection*{Big Idea}

Le but de cette section est d'essayer de séparer les états de transition
en classe. Cette séparation est assez pratique, car tous les états d'une
même classe ont les mêmes propriétés.

Un autre concept abordé est la notion de temps de ruine, qui représente
le nombre d'étapes après lequel on ne peut plus jouer. On verra qu'il
est impossible de calculer l'espérance du temps de ruine pour une v.a.
infinie, car on ne peut pas calculer sa distribution, mais si on a
une chaine de Markov, on peut utiliser ces propriétés pour estimer ce
temps de ruine. Finalement, on a finit avec quelques exemples.

\subsection*{Outline}
\begin{enumerate}
    \item Classification des états: types de classes et propriétés
    \item Temps de Ruine
    \item Périodicité
\end{enumerate}

\subsection{Classification des états: types de classes et propriétés}

\subsubsection{Types de Classes}%
\label{ssub:Types de Classes}

Le premier concept à se souvenir est que la notion d'état et de classe
est similaire. Puisque tous les états d'une même classe possèdent les
mêmes propriétés, alors parler des propriétés d'une classe ou d'un état
revient au même.

Il existe plusieurs types d'états/classe:
\begin{enumerate}
    \item Classe Transitoire vs Classe Récurrente:
	\begin{itemize}
	    \item Transitoire: On peut rester dans cette classe ou y sortir
		$ 0 < P_{j,j} < 1 $
	    \item Récurrente: Une fois qu'on entre dans cette classe, on
		ne peut plus y sortir $ p_{j,j} = 1 $
	\end{itemize}
    \item Classe Fermée vs Classe ouverte:
	\begin{itemize}
	    \item Fermée: on ne peut que reach des états à l'intérieur de
		la classe
	    \item Ouverte:
	\end{itemize}
    \item
\end{enumerate}

\subsubsection{Propriétés des Classes}%
\label{ssub:Propriétés des Classes}

\begin{enumerate}
    \item Si l'état j est absorbant, alors elle est forme sa propre classe
	et on dit que c'est une classe récurrente
    \item Si la classe est fermée, alors c'est une classe récurrente
    \item Un état est transient si $ i \to j$, mais $ j \not\to i$
    \item Si l'espace d'états est fini (il y a un nombre fini d'états),
	alors il y a au moins une classe récurrente
\end{enumerate}

\begin{remark}
    \begin{enumerate}
	\item Pour montrer qu'une classe est transiente, on n'a qu'à montrer
	    la propriété (3) pour l'un des états de la classe
        \item Si on a deux classes et qu'on montre que l'une d'entre elles
	    est transiente, l'autre est nécessairement récurrent par (4)
    \end{enumerate}
\end{remark}

\subsection{Temps de Ruine}

Le temps de ruine correspond au nombre d'étapes après laquelle on ne peut
plus jouer (on get stuck dans un espace récurrent?). Plus généralement,
on a 2 buts:
\begin{enumerate}
    \item Déterminer la probabilité de se faire ruiner en n étapes:
	possible de calculer pour un petit nombre d'étape
    \item Détermine l'espérance du temps de ruine: impossible à calculer
	puisqu'on n'a pas la fonction densité (pcq v.a. infinie)
\end{enumerate}

\subsection{Périodicité}

\begin{definition}[Période d'une classe]
    La période d'une classe est le pgcd de tous les temps $n \geq 1$
    vérifiant $p_{j,j} >0$. Autrement dit, on
    \begin{enumerate}
        \item identifie tous les chemins qui nous permettent de revenir
	    à l'état initial j
	\item compte le nombre d'étapes pour chaque chemin
	\item faire le pgcd entre ces nombres d'étapes
    \end{enumerate}
\end{definition}

\begin{remark}[Classe Apériodique]
    Si la période d'une classe est de 1, on dit que la classe est
    apériodique. Ainsi, une classe récurrente est apériodique
\end{remark}

\begin{remark}
    On dit que la période est une propriété de classe, car les états
    et la classe ont la même période ie on peut interchanger l'utilisation
    de ces termes.
\end{remark}

\subsection{Théorie 2}
\subsection*{Big Idea}

Cette classe-ci se concentrait sur la notion de distribution stationnaire, qui
représente la probabilité de transitionner vers l'état $x_i$ à long terme.
On peut représenter cette probabilité par un vecteur de probabilité $\pi$, qui
est une probabilité stationnaire. Notre but sera de déterminer la probabilité
stationnaire représentant la probabilité de transitionner d'un état à long terme.

\subsection*{Outline}
\begin{enumerate}
    \item Retour sur les classes de transition
    \item Distribution Stationnaire
\end{enumerate}

\subsubsection{Retour sur les classes de transition}

On a fait l'exemple 4 avec lequel on avait 2 classes de récurrence et une
classe de transition. Encore une fois, pour montrer qu'une classe est
récurrente,on doit montrer que  la probabilité de retour est de 1 $p_{j,j} = 1$.
Pour montrer qu'une classe est transitive, on doit montrer que
\begin{enumerate}
    \item Un état de la classe n'est pas symétrique à celui d'une autre classe:
	$ i \to j \not\Longrightarrow j \to i$ ou
    \item La proabilité de non retour n'est pas égale à 1 : $0<p_{j,j} <1$
\end{enumerate}

Pour faire déterminer la périodicité de la classe, on fait le pgcd des chemins
de retour. Si la périodicité est de 1 (pgcd=1), on doit que la classe est
apériodique.

\subsubsection{Distribution Stationnaire}

\textbf{Qu'est-ce qu<une distribution stationnaire}

La distribution stationnaire sert à décrire l'équilibre du système à long
terme. On veut déterminer la probabilité de finir dans l'état $x_i$, peu
importe dans quel état on commence. On représente la probabilité de finir
dans chaque vecteur par $\pi=(\pi _1, ..., \pi _n)$, qu'on nomme vecteur
probabilité, où $\pi _i$ est la probabilité de finir à l'état $x_i$.

\textbf{Comment calculer la probabilité stationnaire de $\pi$}

Pour déterminer la valeur de l'état à long terme, on doit calculer la
probabilité stationnaire par un produit matriciel $\pi= \pi \cdot P$, où P
est la matrice de transition. Intuitivement, on veut veut que la probabilité
du vecteur à un moment donné et celui à un temps très loin soit la même, car
connaitre ce qu'il s'est passé il y a 3000 ans ne change pas vraiment ce qu'on
a aujourd'hui.

Pour ce faire, on veut trouver les paramètres $\pi _i$ en faisant la
multiplication matricielle:
$$  \pi = (\pi _1, \pi _2, \pi _3) \cdot P = (\pi _1, \pi _2, \pi _3)
\begin{bmatrix}
    1/4 & 3/4 & 0\\
    1/4 & 1/2 & 1/4\\
    1/4 & 1/4 & 1/2\\
\end{bmatrix} $$

Cependant, si on résout cette matrice, on obtient une infinité de solutions.
Ainsi, on doit utiliser la propriété markovienne nous disant que la somme des
probabilités de chaque ligne est 1
$$ \sum^{3}_{k=1} \pi _k = 1$$

\begin{remark}[Pourquoi la distribution stationnaire génère une infinité de
    solutions]
    La matrice de probabilité stationnaire génére une infinité de solutions,
    car les vecteurs colonnes ne sont pas linéairement indépendantes, ce qui
    veut dire qu'on peut écrire chaque vecteur comme une combinaison linéaire
    des autres. Ainsi, on ne peut pas associer de pivot à chaque colonne. Par
    conséquent, on a une variable libre.
\end{remark}

\textbf{Que faire si l'espace d'états est infini}

Si l'espace d'état est infini, il se peut que la probabilité stationnaire
n'existe pas. Pour vérifier si elle existe, on n'a qu'à faire le produit
matriciel (si possible), et vérifier que la somme des probabilités de chaque
ligne est de 1

\subsection{TP}

Pas de TP (fête de la Reine)

\pagebreak
\section{Semaine 4}
\subsection{Théorie 1}
TODO
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item Théorème de Convergence
    \item Début du Théorème Ergodique
\end{enumerate}
\subsection{Théorie 2}

Quiz 1

\subsection{TP 3 - Distribution Stationnaire}

Le TP 3 portait sur les distributions stationnaires et sur la classification
des classes

\begin{enumerate}
    \item Représenter le graphe associé pour classifier les états et déterminer
	si la classe est irréductible
    \item Déterminer la lois stationnaire
\end{enumerate}

\subsection{Représenter le graphe associé pour classifier les états}

Il faut se souvenir des justifications suivantes:
\begin{enumerate}
    \item Apériodique: pgcd=1 (à cause des boucles s'il y en a)
    \item Réccurente: $P_{ii}=1$: La probabilité de retour est certaine
    \item Irréductible: La CM finie forme une unique classe + thm: CM finie a
	au moins une classe récurrente.
\end{enumerate}

\subsection{Déterminer la/les lois stationnaires}

La loi stationnaire se trouve en résolvant $ \pi = \pi \cdot P$ et
$\sum \pi _i =1$

Par contre, il faut se souvenir que s'il y a plusieurs classes récurrentes,
alors la loi n'est pas unique et dépend de la classe de départ. Si l'état de
départ est un état transitoire, alors on doit considérer qu'on a autant de
chance d'aller dans un état transitoire qu'un autre. Par contre, si on commence
dans un état récurrente, il faut considérer qu'on ne reste qu'à l'intérieur de
cette classe.

\pagebreak
\section{Semaine 5}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item Retour sur le Théorème de Convergence
    \item Details sur le Théorème Ergodique
    \item Théorème 3: Combinaison Théorème Convergence et Ergodique
\end{enumerate}

\subsection{Retour sur le Théorème de Convergence}

\subsection{Théorème de Convergence}%
\label{sub:Théorème de Convergence}

Le théorème de convergence nous dit que si une chaine de Markov est
irréductible et apériodique, alors $\lim_{n \to \infty} P_{ij} ^{(n)} =
\pi _j$. En d'autres mots, si la CM est irréductible et apériodique,
alors la proportion du temps passé dans chaque état est donné par la
distribution stationnaire (si elle existe)

\subsubsection{Importance des hypothèses du Théorème de Convergence}%
\label{ssub:Importance des hypothèses du Théorème de Convergence}

On s'est aussi penché sur l'importance des hypothèses du théorème de
convergence
\begin{enumerate}
    \item Irréductible: Si la CM n'est pas irréductible, alors $i \not\to j$
	et $P_ij >0$, ce qui contredit le fait que la probabilité de
	retour soit de 1
    \item Apériodique: Si la CM est périodique (pgcd $I_n>1$), alors
	la limite ne pourrait ne pas exister, car la suite est alternée
    \item Existence de la distribution stationnaire
\end{enumerate}

\subsection{Details sur le Théorème Ergodique}

\subsubsection{Théorème Ergodique}%
\label{ssub:Théorème Ergodique}

Le théorème Ergodique dit que pour une chaine de Markov irréductible,
CM admet une distribution stationnaire $\Longleftrightarrow$ elle est
récurrente positive, et
$$ \mathbb{E} [T_j] = \frac{1}{\pi _j} $$

En d'autres mots, on est capable d'estimer l'espérance du nombre d'étapes
pour retourner à l'état i à l'aide de la distribution stationnaire d'une
chaine de Markov est irréductible et est récurrente positive. On est
donc capable de calculer l'espérance de premier retour sans avoir la
distribution de $T_j$

Plus généralement, si CM irréductible, alors
$$ \text{ Existence de $\pi$ } \Longleftrightarrow \text{Récurrente positive et } \mathbb{E} [T_j] = \frac{1}{\pi _j} $$

\begin{remark}[Récurrente positive vs Récurrente nulle]
    \begin{enumerate}
	\item CM positive: $\mathbb{E} [T_j]<\infty$
	\item CM nulle: $\mathbb{E} [T_j]=\infty$
    \end{enumerate}
\end{remark}

\subsection{Théorème 3: Combinaison Théorème Convergence et Ergodique}

Le théorème 3 est la combinaison du théorème de convergenve et du théorème
ergodique. Si les hypothèses des théorèmes 1 et 2 sont respectées (ie
irréductible et ergodique: apériodique et réccurente), alors la
loi stationnaire existe et est unique, et on a
$$ \pi _j = \lim_{n \to \infty} P_{ij}^{(n)} = \frac{1}{\mathbb{E} [T_j]} $$

\subsubsection{Marche Aléatoire Réfléchie}%
\label{ssub:Marche Aléatoire Réfléchie}

Une marche aléatoire réfléchie est représentée par la fonction suivante:
\[
    p_{ij} = \begin{cases}
	\text{p} &\quad\text{ if j=i+1 }\\
	\text{1-p} &\quad\text{ if j=i-1 et $p_{0,1}$ = 1}\\
	\text{0} &\quad\text{ otherwise }\\
    \end{cases}
\]

Ainsi, on peut représenter la marche aléatoire par une somme de Bernouilli
(variable indicatrice): $ X_n = X_0 + Y_1 + ... + Y_n$. En divisant par
le nombre d'essais n de chaque côté, on obtient
$\frac{X_n}{n} = \frac{X_0}{n} + \frac{Y_1 + ...+Y_n}{n} $

Or, puisque l'état initial est une constante, $\lim_{n \to \infty} \frac{X_0}{n} =0$, alors que $ \frac{Y_1 + ...+Y_n}{n} = E[Y_1] = 1(p) + (1-p)(-1) = 2p-1 $. Ainsi, $X_n ~ (2p-1) \cdot n$

On obtient donc 2 cas:
\begin{enumerate}
    \item $2p-1>0: p>1/2$: on a $\lim_{n \to \infty} X_n ~ (2p-1) \cdot n = \infty $: puisque $x_n$ tend vers l'infini, il existe un $n_0$ après lequel
	on ne revient jamais vers l'état de départ (définition
	topologique de la limite à l'infini). \\ $\Longleftrightarrow$
	Ainsi, puisque la probabilité de non-retour n'est pas 1, l'état
	i est transient.\\ $\Longleftrightarrow$ Puisque la CM est
	irréductible, alors la classe est transiente\\
	$\Longleftrightarrow$ la probabilité stationnaire n'existe pas
	(théorème ergodique)
    \item $2p-1<0: p<1/2$: $\lim_{n \to \infty} X_n ~ (2p-1) \cdot n = -\infty $: par définition topologique de la limite à moins l'infini, il
	existe un $n_0$ pour lequel on ne peut pas dépasser. Ainsi,
	la probabilité de retour est de 1. Donc, l'état est récurrent.
	Il ne reste juste qu'à vérifier si la probabilité stationnaire
	$\pi$ existe pour savoir si la classe est récurrente positive ou
	nulle.
\end{enumerate}

\subsection{Théorie 2}
\subsection*{Big Idea}

Ce cours-ci portait sur les marches aléatoires réfléchies infinie.
Auparavant, on ne considérait que les états finis, et on savait que
si la classe était irréductible, alors elle était récurrente positive.
Cependant, lorsqu'on a une marche aléatoire infinie, il faut considérer 2
cas:
\begin{enumerate}
    \item p>1/2: Puisque $X_n$ tend vers l'infini, alors la probabilité
	de non retour à l'état i >0, ce qui implique que la classe est
	transiente et que la distribution stationnaire n'existe pas
	(par théorème ergodique)
    \item p<1/2: Puisque $X_n$ tend vers moins l'infini, alors la
	probabilité de retour est de 1, et puisque la CM est irréductible,
	alors la classe est récurrente. Pour déterminer si la classe est
	récurrente positive (ou nulle), il faut vérifier que la
	distribution stationnaire existe.
\end{enumerate}

Ce cours-ci porte donc sur la preuve du 2e cas: on veut montrer que
la distribution stationnaire existe et que la classe est récurrente
positive.

\subsection*{Outline}
\begin{enumerate}
    \item Preuve de la convergence d'une marche aléatoire si p<1/2
\end{enumerate}

\subsection{Preuve de la convergence d'une marche aléatoire si p<1/2}

Notons que la matrice de transition d'une marche aléatoire réfléchie
est donnée par
$ P = \begin{bmatrix}
    0 & 1 & 0 & ... & 0\\
    1-p & 0 & p & ... & 0\\
    0 & 1-p & 0 & ... & 0\\
    ... & ... & ... & ... & ...
\end{bmatrix}$

Ainsi, on doit résoudre le SEL suivant:
\begin{enumerate}
    \item $\pi _0 = (1-p) \pi_1$
    \item $\pi _1 = \pi_ 0 + (1-p) \pi_2$
    \item $\pi _2 = \pi_ 1 + (1-p) \pi_3$
    \item ...
    \item $\pi _n = \pi_{n-1} + (1-p) \pi_{n+1}$
\end{enumerate}

En résolvant et en exprimant chacune des lignes en termes de $\pi _0$,
on obtient la relation de récurrence suivante:
$ \pi_n = \frac{p^{n-1}}{(1-p)^n} \cdot \pi_0$

On n'a qu'à vérifier ce résultat par induction en partant de l'hypothèse
de base $ \pi_n = \frac{p^{n-1}}{(1-p)^n} \cdot \pi_0$ et en isolant
$\pi_{n+1}$ pour montrer que n+1 est vraie: $\pi_{n+1} = \frac{p^n}{(1-p)^{n+1}} $

\subsection{TP 4 - Théorèmes de Convergence et Ergodique}

Le 4e TP portait sur 3 sujets:
\begin{enumerate}
    \item Utilisation du théorème de convergence et du théorème ergodique
    \item Bonus: Chaines de Markov Réversibles
\end{enumerate}

\subsubsection{Utilisation du théorème de convergence et du théorème ergodique}

Il faut se rappeler que le théorème de convergence nous dit la chose
suivante: si la chaine de Markov est irréductible, apériodique et admet une
distribution stationnaire, alors la probabilité à long terme est donné par
la distribution stationnaire. On note
$ \lim_{n \to \infty} P_{0k} ^{(n)} = \pi$, où
$\pi = \pi \cdot P$ et $\sum \pi _i = 1$

Le théorème ergodique nous dit que si la Chaine de Markov est ergodique
(irréductible, apériodique et récurrente positive), alors l'espérance
du temps de retour est donné par $\mu _i = E[T_i] = \frac{1}{\pi _i}$

On cherche à répondre aux questions suivantes:
\begin{enumerate}
    \item Trouver la distribution stationnaire: Théorème de Convergence
    \item Quelle est la probabilité de se retrouver à l'état i à long
	terme: Trouver la distribution stationnaire
    \item Trouver l'espérance du temps de retour: théorème ergodique
\end{enumerate}

\subsubsection{Problème: Probabilité que $S_n$ soit divisible par 6}%
\label{ssub:Problème: Probabilité que $S_n$ soit divisible par 6}

Considérons un dé à 6 faces. On veut déterminer la probabilité que la
somme des n premiers lancers de dé est divisible par 6. What if n tend vers
l'infini.

On doit considérer $X_n = S_n (mod 6)$

\subsubsection{Bonus: Chaines de Markov Réversibles}

Une chaine de Markov est réversible ssi la probabilité de traverser un cycle
est le même dans les 2 sens. On dit que la chaine de Markov admet un
réseau de conductances. Justification: Puisque $c_{ij} =c_{ji}$, alors
$m_i p_{ij} = n_j p_{ji} \Longrightarrow$ la chaine est réversible

La probabilité de transition est donné par le poids de l'arc:
$p_{ij} = \frac{c_{ij}}{m_i} $

On dit note que $(m_i) \propto pi _i$ et puisque $\sum \pi _i =1$, alors
$ \pi _i = \frac{m_i}{M} , M= \sum m_s$

\textbf{Exemple: Probabilité qu'un roi revienne sur la même case}

Un échequier a une dimension de 8x8. On doit remarquer qu'il y a 3 types
de cases:
\begin{enumerate}
    \item coin (x4): 3 options $ \Longrightarrow m_{coin} = 3$
    \item bord (x24): 5 options $ \Longrightarrow m_{bord} = 5$
    \item centre (x36): 8 options $ \Longrightarrow m_{centre} = 8$

\end{enumerate}

Ainsi, $ M= 36 x m_{centre} + 24 x m_{bord} + 4 x m_{coin} = 420$,
et donc $\pi_{coin} = 3/420$, $\pi_{bord}=5/420$, $\pi _{centre}=8/420$,

Donc, en sachant qu'on part du coin, il faut en moyenne 140 coups pour
revenir sur le même coin.

\pagebreak
\section{Semaine 6}
\subsection{Théorie 1}
\subsection*{Big Idea}

Le cours se comportait 2 parties. Dans un premier temps, on voulait montrer que pour une marche aléatoire, lorsque p<1/2, la CM était récurrente
positive, alors la distribution stationnaire existait et était donnée
par une récurrence qu'on montrera

Dans un deuxième temps, on a commencé à parler de CM à temps continu
ie des processus de poisson. On a vu que
\begin{enumerate}
    \item le temps entre 2 évènements est iid
    \item la différence entre deux évènements iid est iid
\end{enumerate}

\subsection*{Outline}
\begin{enumerate}
    \item Convergence d'une Marche aléatoire réfléchie
    \item Introduction aux CM à temps continu: Processus de Poisson
\end{enumerate}

\subsection{Convergence d'une Marche aléatoire réfléchie}

Étapes:
\begin{enumerate}
    \item Trouver la fonction de densité représentant la marche
	aléatoire en l'écrivant comme somme de v.a Bernouilli
    \item Trouver les valeurs de p pour lesquelles la marche aléatoire
	est transitive (p<1/2), récurrente nulle (p=1/2) et récurrente
	positive (p>1/2)
    \item Trouver la distribution stationnaire lorsque p<1/2
	\begin{itemize}
	    \item Écrire la matrice de transition pour trouver $\pi _i$
	    \item Exprimer $\pi _n$ par récurrence:
		$\pi _n = p \pi _{n-1} +(1-p) \pi _{n+1}$
	    \item Exprimer $\pi _n$ en fonction de $\pi _0$ en trouvant
		la récurrence donnée en résolvant les premières variables
		du SEL: $ \pi _n = \frac{p^{n-1}}{(1-p)^n} \pi _0$
	    \item Prouver par induction que la formule trouvée est bonne
	    \item Déduire $\pi _n$ en terme de p: on utilise le fait
		que $\sum \pi _i =1$ et on remarque une série géométrique
	\end{itemize}
\end{enumerate}

On peut représenter notre marche aléatoire réfléchie comme une somme
de variable indicatrice (Bernouilli): $X_n = X_0 + Y_1 + ... + Y_n$
Ainsi, l'espérance de la marche aléatoire à chaque pas ie la distance
à laquelle on se déplace à chaque step, est donnée par
$$ \frac{X_n}{n} = \frac{X_0}{n} + \frac{Y_1 + ...+ Y_n}{n} $$

Puisque $X_0$ est une constante, alors elle converge vers 0. Puisque
$Y_i$ est une Bernoulli, alors $E[Y_i] = (1)p+(-1)(1-p)$. Ainsi,
$X_n ~ (2p-1) \cdot n$

On veut montrer que:
\begin{enumerate}
    \item si p>1/2: la marche aléatoire diverge, car
	$\lim_{n \to \infty} X_n = \infty $, alors il existe un M pour
	lequel $X_n>M$ donc la probabilité de non-retour >1. Ainsi,
	la classe est transitive
    \item si p=1/2: la classe est récurrente nulle:
	$ \lim_{n \to \infty} = 0 $
    \item si p<1/2: la classe est récurrente positive, car
	$\lim_{n \to \infty} X_n = -\infty $, alors la probabilité de
	retour >0,
\end{enumerate}

\subsection{Introduction aux CM à temps continu: Processus de Poisson}

TODO

\subsection{Théorie 2}

Quiz 2

\subsection{TP 5 - Marche aléatoire et Processus de Poisson}

TODO

\pagebreak
\section{Semaine 7}
NEXT
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Intra}

\pagebreak
\section{Semaine 8}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 9}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 10}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 11}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 12}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 13}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 14}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 15}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 16}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\end{document}
\end{article}
