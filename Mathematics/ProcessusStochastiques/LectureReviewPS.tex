\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{parskip}
\usepackage{textgreek}
\begin{document}
\title{Résumé des Cours - Processus Stochastiques}
\author{Emulie Chhor}
\maketitle

\section*{Introduction}

Ce document est un résumé des idées importantes présentées lors de chaque cours
de MAT2717

\newtheorem{definition}{Definition}[subsection]
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{corollary}{Corollary}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{axiom}{Axiome}
\newtheorem{property}{Propriété}[subsection]
\newtheorem*{remark}{Remarque}
\newtheorem*{problem}{Problème}
\newtheorem*{intuition}{Intuition}

% \renewcommand*{idea}{Big Big Idea}
% \renewcommand*{outline}{Outline}

\pagebreak
\section{Semaine 1}
\subsection{Théorie 1}

\subsection*{Big Idea}

Dans le premier cours de probabilité, on s'est penché sur la notion de variable
aléatoire. Dans ce cours-ci, on voit qu'on doit ajouter un autre paramètre à
ces variables aléatoires: le temps. On ne parle donc plus de variables aléaoires,
mais de processus stochastiques, qui sont des variables aléatoires prenant en
considération le temps. Ainsi, on parle donc de processus stochastique à temps
discrete et à temps continu, dépendemment si on considère le temps discret ou
continu.

\subsection*{Outline}
\begin{enumerate}
    \item Processus Stochastiques
    \item Marche Aléatoire
\end{enumerate}

\subsection{Processus Stochastiques}

Un processus stochastique est une v.a aléatoire qui est dépendante du temps.
Elle peut être discrète ou continue dépendemment à quelle fréquence on
observe la valeur de la v.a)\\

On dit que le temps est déterministe. Why?\\

On peut écrire un p.s. $X( \omega, t )$, avec omega: les scenarios
possibles et t, le temps. Si on fixe le temps, on parle de variable
aléatoire. Si on fixe l'évènement et on laisse le temps libre, on
parle de trajectoire.

\subsection{Marche Aléatoire}

La marche aléatoire représente la valeur d'un p.s. discret à un temps donnée.
On peut la modéliser de deux façons:
\begin{enumerate}
    \item Conditionnement: valeur à la dernière position $\pm$ 1
    \item Somme de variables indicatrices: $X_n = Y_1 + ... +Y_n$
\end{enumerate}

Il est à noter qu'on préfère représenter une marche aléatoire par une
somme de variable indicatrice puisque ça rend le calcul plus facile.

Espérance: $\mathbb{E} (X_n) = n (2p-1)$
Variance: $ Var(X_n)= 4np(1-p)$

\subsection{D'ou vient la formule de l'esperance et de la variance d'une
marche aleatoire}

Il s'agit de l'esperance et de la variance pour une somme de variable
indicatrice.

$E(x) = n(1 * p + (-1) (1-p))$ : nombre de pas x esperance succes/echec au
i-ème pas
$V(X)$ = somme des variances = $ n [ E(Y^2) - E(Y)^2 ]$

\subsection{Théorie 2}
\subsection*{Big Idea}

Ce cours introduisait les chaines de Markov. Comme on l'a vu au dernier
cours, les processus stochastiques sont des v.a. qui sont en fonction du
temps. Certaine PS ont la propriété de Markov, c-à-d qu'on n'a que besoin
de la dernière valeur pour déterminer la prochaine position et non tout
l'historique. On introduit la notion de matrice de transition pour modéliser
la probabilité de transitionner d'un état à l'autre, et la notion d'état
absorbant, qui est un état duquel on ne peut se sortir une fois atteinte.

\subsection*{Outline}

\begin{enumerate}
    \item Rappel: Processus Stochastiques
    \item Marche Aléatoire
    \item Processus de Branchement
    \item Chaines de Markov: Proba de transition et État absorbant
\end{enumerate}

\subsubsection{Marche Aléatoire}%
\label{ssub:marche_aléatoire}

Il faut se rappeler que la marche aléatoire sert à modéliser un p.s discret.
On a vu qu'il y avait deux façons de l'interpréter:
\begin{enumerate}
    \item Conditionnement: previous +/- 1 dépendemment du succès/échec
    \item Somme de variables indicatrices (bernouilli)
\end{enumerate}

Essentiellement, la probabilité est déterminer par une Bernouilli, puisqu'on
considère que chaque coin flip est iid. On peut aussi visualiser la
marche aléatoire comme un arbre de décision.\\

Exemple: t=2, coin flip: +1 si Head, -1 si Tail
\begin{enumerate}
    \item $x_2 = 2$ avec proba $p^2$ : {HH}
    \item $x_2 = 0$ avec proba $ 2p (1-p)$ : {HT, TH}
    \item $x_2 = -2$ avec proba $ (1-p)^2$ : {TT}
\end{enumerate}

Notons qu'après 2 coins flip, on ne peut pas avoir 1 ou -1, car apres le
premier coin flip, on est à 1 ou -1, et on ne peut que se déplacer de 1
vers le haut ou vers le bas

On a aussi vu qu'on peut tracer la trajectoire de la marche aléatoire, qui
représente la valeur de $x_n$ selon le temps.

\subsubsection{Processus de Branchement}%
\label{ssub:processus_de_branchement}

Le processus de Banchament est une façon de modéliser la générations
d'évènement. Par exemple, en reinforcement learning, ça représente la
transition d'état. Visuellement, on a un arbre et chaque branche mène
à un autre état, associé à une certaine probabilité.

\subsubsection{Chaine de Markov}%
\label{ssub:chaine_de_markov}

Comme mentionné, une chaine de Markov est une p.s qui satisfait à la
propriété de Markov, c-à-d qu'on n'a que besoin de l'état précédant pour
déterminer le futur et non tout l'historique. Dans la vraie vie, une
chaine de markov ne représente pas nécessairement la situation, mais
on fait cette hypothèse afin de simplifier les calculs et éviter de
trainer trop d'information. On écrit:
$$ \mathbb{P} (X_{n+1} = j) = (X_{n+1} = j | X_n + ... + X_0) =
(X_{n+1} = j | X_n) $$

Pour représenter la probabilité de transition d'un état à un autre, on
utilise une matrice de transition.

Finalement, il est important de comprendre la notion d'état absorbant.
Une fois qu'on entre dans cet état, on ne peut plus en sortir. L'exemple
donné en classe concernait le casino: une fois qu'on a plus d'argent à
gamble, on ne peut plus gagner d'argent et on reste à zéro.

Notons que si une chaine de Markov à un état absorbant, il est fort
probable qu'il y ait plusieurs états absorbant et qu'on puisse observer
un pattern.

\subsection{TP}

Cette semaine, le TP se penchait sur 2 sujets:
\begin{enumerate}
    \item Rappel sur les concepts en probabilité
    \item Intro aux Processus Stochastiques
\end{enumerate}

\subsubsection{{Rappel sur les concepts en probabilité}}

\begin{enumerate}
    \item Principe Inclusion-Exclusion
    \item Probilités Totales pour généraliser le nombre de tirage avec et sans
	remise: $ P(A_1|B) P(B) + P(A_2|B)P(B) $
    \item Trouver la densité marginale d'une v.a: si continues, on doit essayer
	de retrouver une fonction de densité pour éviter d'avoir à résoudre
	l'intégrale
\end{enumerate}

\subsubsection{Intro aux Processus Stochastiques}

\begin{enumerate}
    \item Déterminer si une p.s. est Markov ou non
    \item Trouver/Lire une Matrice de Transition
    \item Déterminer la probabilité de transition d'un état à un autre
    \item Tracer le Graphe
\end{enumerate}

\subsection{Intro aux Processus Stochastiques}

\pagebreak

\section{Semaine 2}
\subsection{Théorie 1: Probabilité de Transition à plusieurs étapes}
\subsection*{Big Idea}

Ce cours-ci nous présentait certaines méthodologies pour déterminer
la probabilité de transition à plusieurs étapes pour des P.S discrèts
à temps homogène satisfaisant la propriété de Markov

\subsection*{Outline}
\begin{enumerate}
    \item Modèles: Marche aléatoire, Processus de Branchement, Modèle de
	diffusion de Gaz
    \item Probabilité de transition avec probabilité conditionnelle
    \item Probabilité de transition avec Relation Chapman-Kolmogrov
\end{enumerate}

\subsubsection{Modèles: Marche aléatoire, Processus de Branchement, Modèle de    diffusion de Gaz}

Dans un premier temps, on a réviser les notions de marche aléatoire,
processus de branchement et  modèle de diffusion de gaz. Essentiellement,
on voulait s'apercevoir que
\begin{enumerate}
    \item La valeur de $X_n$ ne peut que se déplacer de 1 (dans la plupart
	du temps)
    \item Conditionnement: on peut trouver la probabilité de transition
	d'un état à l'autre par conditionnement p/r à l'étape précédante
\end{enumerate}

\subsubsection{Probabilité de transition avec probabilité conditionnelle}

Dans les cours précédants, on nous donnait la matrice de transition pour
déterminer la probabilité de se déplacer d'un état à l'autre. Ici, on
voit 2 résultats importants:
\begin{enumerate}
    \item si P.S Markov et homogène (même proba peu import le temps),
	alors $$ P[X_2 = 3 , X_1 = 2 | X_0 = 1] = P[X_{n+2} = 3, X_{n+1} = 2
	| X_n =1] $$, ce qui veut dire que la proba de transition d'état ne
	dépend pas d'un temps quelconque, mais que de la "distance" (nombres
	de pas) entre 2 états, si P.S Markov est homogène
    \item On peut utiliser la probabilité conditionnelle pour déterminer la
	proba de transition entre 2 états
\end{enumerate}

\textbf{Preuve pour 1}

Puisqu'on a une chaine de Markov, les les 2 états sont indépendants et on peut
utiliser la règle de multiplication:

$$ P[X_2 = 3 , X_1 = 2 | X_0 = 1] $$
$$ = \frac{P(X_2 = 3, X_1 =2, X_0 = 1)}{P(X_0 = 1)}  $$
$$ = (\frac{P(X_2 = 3, X_1 =2, X_0 = 1)}{P(X_0 = 1)})
(\frac{P(X_1 = 3, X_1 = 2)}{P(X_0=1)} ) $$
$$ ... $$

\textbf{Probabilité Conditionnelle pour trouver la probabilité de transition}

Plus généralement, pour trouver la probabilité de transition à plusieurs
étapes, on doit conditionner par les étapes intermédiaires:

\begin{enumerate}
    \item 2 étapes: $ P(X_2 = 1 | X_0 =1) = \sum^{n}_{i=1} P(X_2 = 2, X_1 = k | X_0=1) $
    \item 3 étapes: $ P(X_3 = 1, X_2 = 1 | X_0 =1) = P(X_3 = 1, X_2 = 1)
	P(X_2 = 1 | X_1 =1) P(X_1 = 1 | X_0 =2)$
    \item m étapes: $ P(X_{n+m} = j | X_n = i) = P(n=j |X_0 = i) $
\end{enumerate}

\subsubsection{Probabilité de transition avec Relation Chapman-Kolmogrov}

La relation de Chapman-Kolmogrov est pratique, car elle nous permet de généraliser
le calcul pour la probabilité de transition en m étapes. Au lieu de multiplier
individuellement les intermédiaires à chaque étapes, on peut utiliser la
multiplication matricielle pour obtenir la matrice de transition après m
étapes. On note $ P^(m) = P^m $, P: matrice de transition.\\

Par exemple, si on veut la proba de transition à 3 étapes, on n'a qu'à faire
$$ P \times P \times P $$

\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item Preuve de la Relation Chapman-Kolmogrov
    \item Classes: Communication entre états
\end{enumerate}

\subsubsection{Preuve de la Relation Chapman-Kolmogrov}

La relation de Chapman-Kolmogov nous dit que si on a une chaine de
Markov à temps homogène, on peut multiplier les matrices de transition
pour obtenir la probabilité de passer de l'état n à l'état n+k
\begin{enumerate}
    \item $ P^{m}=P^m$
    \item $ P^{m+n}=P^m P^n$
\end{enumerate}

La preuve de cette relation se fait par induction, et on montre
l'étape d'induction avec la propriété des chaines de Markov et la
probabilité conditionnelle. Essentiellement, on veut montrer qu'en
multipliant chaque entrées entre elles, on obtient la proba à l'étape
n x la proba à l'étape 1 et qu'en multipliant ces matrices, on obtient
$ P^{n+1}$

\textbf{Cas de base}

\begin{enumerate}
    \item t=0: matrice identité, car on ne change pas d'état
    \item t=1: $P^1 = P$
\end{enumerate}

\textbf{Étape d'induction}

\begin{enumerate}
    \item Puisque CM est homogène dans le temps, alors
	$ P^{n+1} \Longrightarrow P_{ij} ^{n+1} = P[X_{n+1} = j | X_n = i]
	= P[X_{n+1} =j | X_0 = i] $
    \item En considèrant les états intermédiaires, on obtient
	$ P[X_{n+1}=j | X_0 = i] = \sum^{n}_{i=1} P[X_{n+1} = j | X_n = k]$
    \item Puisque c'est une chaine de Markov, on a que
	$$ \sum^{n}_{i=1} P[X_{n+1} = j, X_n = k | X_0 = i]$$
    \item Par probabilité conditionnelle, on a que
	$$ \sum^{n}_{i=1} \frac{P[X_{n+1} = j, X_n = k, X_0 = i]}
	{P[X_0=i]} \frac{P[X_n = k, X_0=i]}{P[X_n = k, X_0=i]} $$
    \item En réarrangeant les termes, on obtient la proba de transition
	à l'étape n et en une étape
	$$ \sum^{n}_{i=1} \frac{P[X_{n+1} = j, X_n = k, X_0 = i]}
	{P[X_n = k, X_0=i]} \frac{P[X_n = k, X_0=i]}{P[X_0=i]} $$
    \item et on applique la définition de la multiplication matricielle
\end{enumerate}

\subsubsection{Classes: Communication entre états}

La deuxième partie du cours mettait l'emphase sur la séparation des
états par des classes. Essentiellement, on veut être capable de
différentier les différentes classes dans une matrice de transition
afin de déterminer si la matrice est irréductible ou non.

\textbf{Qu'est-ce qu'une classe}

Tous les états qui communiquent entre eux forment une classe. Pour que
2 états communiquent entre entre elles, il faut qu'elles satisfassent
les conditions suivantes:
\begin{enumerate}
    \item Réflexive: $ i \leftrightarrow i$
    \item Symétrique: $ i \leftrightarrow j \Longleftrightarrow j
	\leftrightarrow i $
    \item Transitive: $ i \to j, j \to k \Longrightarrow i \to k $
\end{enumerate}

\textbf{Quelles sont les types de classes}

\begin{enumerate}
    \item État absorbant: On ne peut jamais quitter cet état après y
	être entré
    \item État de non-retour: une fois qu'on n'a quitté cet état,
	on ne peut jamais revenir
\end{enumerate}

\textbf{Qu'est-ce qu'une matrice irréductible}

On dit qu'une matrice est irréductible si tous les états communiquent
entre elles. Autrement dit, tous les états forment une seule classe.

\subsection{TP}

Le TP de cette semaine se penchait sur 2 buts:
\begin{enumerate}
    \item Déterminer la probabilité de passer d'un état à l'autre
    \item Classification des états
\end{enumerate}

\subsection{Déterminer la probabilité de passer d'un état à l'autre}

Pour déterminer la probabilité de passer d'un état à l'autre, on a deux
façons:
\begin{enumerate}
    \item si le nombre de steps est relativement petit: conditionnement
	(tracer l'arbre peut aider)
    \item si le nombre de steps est grand: utiliser la relation de
	Chapman-Kolmogrov pour calculer la matrice de transition en
	diagonalisant
\end{enumerate}

\subsection{Classification des états}

Dans une matrice de transition, on veut être capable de regrouper les
états similaires afin de créer de classes. Tous les états dans une
même classe devraient pouvoir être accessible, c-à-d qu'il existe un
chemin entre les 2. Les classes peuvent être:
\begin{enumerate}
    \item Transition vs Récurrent: on pourrait quitter la classe et ne
	jamais y revenir vs une fois qu'on est dans cette classe, on
	ne peut jamais la quitter (non-retour)
    \item Périodique vs Apériodique: après combien de pas en revient
	à l'état d'où on est partit (cycle) vs pgcd = 1
    \item Irréductibilité: le graphe forme une seule classe
\end{enumerate}

\begin{remark}
    Il est utile de tracer le graphe pour déterminer les classes
\end{remark}

\pagebreak
\section{Semaine 3}
\subsection{Théorie 1}
\subsection*{Big Idea}

Le but de cette section est d'essayer de séparer les états de transition
en classe. Cette séparation est assez pratique, car tous les états d'une
même classe ont les mêmes propriétés.

Un autre concept abordé est la notion de temps de ruine, qui représente
le nombre d'étapes après lequel on ne peut plus jouer. On verra qu'il
est impossible de calculer l'espérance du temps de ruine pour une v.a.
infinie, car on ne peut pas calculer sa distribution, mais si on a
une chaine de Markov, on peut utiliser ces propriétés pour estimer ce
temps de ruine. Finalement, on a finit avec quelques exemples.

\subsection*{Outline}
\begin{enumerate}
    \item Classification des états: types de classes et propriétés
    \item Temps de Ruine
    \item Périodicité
\end{enumerate}

\subsection{Classification des états: types de classes et propriétés}

\subsubsection{Types de Classes}%
\label{ssub:Types de Classes}

Le premier concept à se souvenir est que la notion d'état et de classe
est similaire. Puisque tous les états d'une même classe possèdent les
mêmes propriétés, alors parler des propriétés d'une classe ou d'un état
revient au même.

Il existe plusieurs types d'états/classe:
\begin{enumerate}
    \item Classe Transitoire vs Classe Récurrente:
	\begin{itemize}
	    \item Transitoire: On peut rester dans cette classe ou y sortir
		$ 0 < P_{j,j} < 1 $
	    \item Récurrente: Une fois qu'on entre dans cette classe, on
		ne peut plus y sortir $ p_{j,j} = 1 $
	\end{itemize}
    \item Classe Fermée vs Classe ouverte:
	\begin{itemize}
	    \item Fermée: on ne peut que reach des états à l'intérieur de
		la classe
	    \item Ouverte:
	\end{itemize}
    \item
\end{enumerate}

\subsubsection{Propriétés des Classes}%
\label{ssub:Propriétés des Classes}

\begin{enumerate}
    \item Si l'état j est absorbant, alors elle est forme sa propre classe
	et on dit que c'est une classe récurrente
    \item Si la classe est fermée, alors c'est une classe récurrente
    \item Un état est transient si $ i \to j$, mais $ j \not\to i$
    \item Si l'espace d'états est fini (il y a un nombre fini d'états),
	alors il y a au moins une classe récurrente
\end{enumerate}

\begin{remark}
    \begin{enumerate}
	\item Pour montrer qu'une classe est transiente, on n'a qu'à montrer
	    la propriété (3) pour l'un des états de la classe
        \item Si on a deux classes et qu'on montre que l'une d'entre elles
	    est transiente, l'autre est nécessairement récurrent par (4)
    \end{enumerate}
\end{remark}

\subsection{Temps de Ruine}

Le temps de ruine correspond au nombre d'étapes après laquelle on ne peut
plus jouer (on get stuck dans un espace récurrent?). Plus généralement,
on a 2 buts:
\begin{enumerate}
    \item Déterminer la probabilité de se faire ruiner en n étapes:
	possible de calculer pour un petit nombre d'étape
    \item Détermine l'espérance du temps de ruine: impossible à calculer
	puisqu'on n'a pas la fonction densité (pcq v.a. infinie)
\end{enumerate}

\subsection{Périodicité}

\begin{definition}[Période d'une classe]
    La période d'une classe est le pgcd de tous les temps $n \geq 1$
    vérifiant $p_{j,j} >0$. Autrement dit, on
    \begin{enumerate}
        \item identifie tous les chemins qui nous permettent de revenir
	    à l'état initial j
	\item compte le nombre d'étapes pour chaque chemin
	\item faire le pgcd entre ces nombres d'étapes
    \end{enumerate}
\end{definition}

\begin{remark}[Classe Apériodique]
    Si la période d'une classe est de 1, on dit que la classe est
    apériodique. Ainsi, une classe récurrente est apériodique
\end{remark}

\begin{remark}
    On dit que la période est une propriété de classe, car les états
    et la classe ont la même période ie on peut interchanger l'utilisation
    de ces termes.
\end{remark}

\subsection{Théorie 2}
\subsection*{Big Idea}

Cette classe-ci se concentrait sur la notion de distribution stationnaire, qui
représente la probabilité de transitionner vers l'état $x_i$ à long terme.
On peut représenter cette probabilité par un vecteur de probabilité $\pi$, qui
est une probabilité stationnaire. Notre but sera de déterminer la probabilité
stationnaire représentant la probabilité de transitionner d'un état à long terme.

\subsection*{Outline}
\begin{enumerate}
    \item Retour sur les classes de transition
    \item Distribution Stationnaire
\end{enumerate}

\subsubsection{Retour sur les classes de transition}

On a fait l'exemple 4 avec lequel on avait 2 classes de récurrence et une
classe de transition. Encore une fois, pour montrer qu'une classe est
récurrente,on doit montrer que  la probabilité de retour est de 1 $p_{j,j} = 1$.
Pour montrer qu'une classe est transitive, on doit montrer que
\begin{enumerate}
    \item Un état de la classe n'est pas symétrique à celui d'une autre classe:
	$ i \to j \not\Longrightarrow j \to i$ ou
    \item La proabilité de non retour n'est pas égale à 1 : $0<p_{j,j} <1$
\end{enumerate}

Pour faire déterminer la périodicité de la classe, on fait le pgcd des chemins
de retour. Si la périodicité est de 1 (pgcd=1), on doit que la classe est
apériodique.

\subsubsection{Distribution Stationnaire}

\textbf{Qu'est-ce qu<une distribution stationnaire}

La distribution stationnaire sert à décrire l'équilibre du système à long
terme. On veut déterminer la probabilité de finir dans l'état $x_i$, peu
importe dans quel état on commence. On représente la probabilité de finir
dans chaque vecteur par $\pi=(\pi _1, ..., \pi _n)$, qu'on nomme vecteur
probabilité, où $\pi _i$ est la probabilité de finir à l'état $x_i$.

\textbf{Comment calculer la probabilité stationnaire de $\pi$}

Pour déterminer la valeur de l'état à long terme, on doit calculer la
probabilité stationnaire par un produit matriciel $\pi= \pi \cdot P$, où P
est la matrice de transition. Intuitivement, on veut veut que la probabilité
du vecteur à un moment donné et celui à un temps très loin soit la même, car
connaitre ce qu'il s'est passé il y a 3000 ans ne change pas vraiment ce qu'on
a aujourd'hui.

Pour ce faire, on veut trouver les paramètres $\pi _i$ en faisant la
multiplication matricielle:
$$  \pi = (\pi _1, \pi _2, \pi _3) \cdot P = (\pi _1, \pi _2, \pi _3)
\begin{bmatrix}
    1/4 & 3/4 & 0\\
    1/4 & 1/2 & 1/4\\
    1/4 & 1/4 & 1/2\\
\end{bmatrix} $$

Cependant, si on résout cette matrice, on obtient une infinité de solutions.
Ainsi, on doit utiliser la propriété markovienne nous disant que la somme des
probabilités de chaque ligne est 1
$$ \sum^{3}_{k=1} \pi _k = 1$$

\begin{remark}[Pourquoi la distribution stationnaire génère une infinité de
    solutions]
    La matrice de probabilité stationnaire génére une infinité de solutions,
    car les vecteurs colonnes ne sont pas linéairement indépendantes, ce qui
    veut dire qu'on peut écrire chaque vecteur comme une combinaison linéaire
    des autres. Ainsi, on ne peut pas associer de pivot à chaque colonne. Par
    conséquent, on a une variable libre.
\end{remark}

\textbf{Que faire si l'espace d'états est infini}

Si l'espace d'état est infini, il se peut que la probabilité stationnaire
n'existe pas. Pour vérifier si elle existe, on n'a qu'à faire le produit
matriciel (si possible), et vérifier que la somme des probabilités de chaque
ligne est de 1

\subsection{TP}

Pas de TP (fête de la Reine)

\pagebreak
\section{Semaine 4}
\subsection{Théorie 1}
TODO
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 5}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 6}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 7}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 8}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 9}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 10}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 11}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 12}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 13}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 14}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 15}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\section{Semaine 16}
\subsection{Théorie 1}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{Théorie 2}
\subsection*{Big Idea}
\subsection*{Outline}
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}
\subsection{TP}

\pagebreak
\end{document}
\end{article}
